{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deepsurv.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "psAL6qNWq8bR",
        "colab_type": "code",
        "outputId": "a40d496e-c3c7-4442-fc0d-338d47c182e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install deepsurv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepsurv\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/55/416bc3610452c54ad3028bfee3a4c69b269a2b9d026be90c7221039222cc/deepsurv-0.1.0.tar.gz\n",
            "Requirement already satisfied: theano in /usr/local/lib/python2.7/dist-packages (from deepsurv) (1.0.4)\n",
            "Collecting lasagne (from deepsurv)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/bf/4b2336e4dbc8c8859c4dd81b1cff18eef2066b4973a1bd2b0ca2e5435f35/Lasagne-0.1.tar.gz (125kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 10.6MB/s \n",
            "\u001b[?25hCollecting lifelines (from deepsurv)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/f4/408183aee57b9625805b02441e761fedf32c29559e4be15823afee375de5/lifelines-0.19.5-py2.py3-none-any.whl (260kB)\n",
            "\u001b[K    100% |████████████████████████████████| 266kB 25.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from theano->deepsurv) (1.16.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from theano->deepsurv) (1.2.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from theano->deepsurv) (1.11.0)\n",
            "Collecting bottleneck>=1.0 (from lifelines->deepsurv)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/ae/cedf5323f398ab4e4ff92d6c431a3e1c6a186f9b41ab3e8258dff786a290/Bottleneck-1.2.1.tar.gz (105kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 29.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: autograd>=1.2 in /usr/local/lib/python2.7/dist-packages (from lifelines->deepsurv) (1.2)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python2.7/dist-packages (from lifelines->deepsurv) (0.23.4)\n",
            "Requirement already satisfied: matplotlib<3.0,>=2.0; python_version < \"3.0\" in /usr/local/lib/python2.7/dist-packages (from lifelines->deepsurv) (2.2.4)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python2.7/dist-packages (from autograd>=1.2->lifelines->deepsurv) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas>=0.18->lifelines->deepsurv) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas>=0.18->lifelines->deepsurv) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib<3.0,>=2.0; python_version < \"3.0\"->lifelines->deepsurv) (0.10.0)\n",
            "Requirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python2.7/dist-packages (from matplotlib<3.0,>=2.0; python_version < \"3.0\"->lifelines->deepsurv) (1.5)\n",
            "Requirement already satisfied: subprocess32 in /usr/local/lib/python2.7/dist-packages (from matplotlib<3.0,>=2.0; python_version < \"3.0\"->lifelines->deepsurv) (3.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib<3.0,>=2.0; python_version < \"3.0\"->lifelines->deepsurv) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib<3.0,>=2.0; python_version < \"3.0\"->lifelines->deepsurv) (2.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<3.0,>=2.0; python_version < \"3.0\"->lifelines->deepsurv) (40.9.0)\n",
            "Building wheels for collected packages: deepsurv, lasagne, bottleneck\n",
            "  Building wheel for deepsurv (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f9/3d/e4/0caa90decee5e102fc1ce763fa98d0021b05aec26a6a4fa552\n",
            "  Building wheel for lasagne (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a5/8e/31/b4cae7e5507f8582e77d7f5cf2815be8820ccacfa0519ca60c\n",
            "  Building wheel for bottleneck (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f2/bf/ec/e0f39aa27001525ad455139ee57ec7d0776fe074dfd78c97e4\n",
            "Successfully built deepsurv lasagne bottleneck\n",
            "Installing collected packages: lasagne, bottleneck, lifelines, deepsurv\n",
            "Successfully installed bottleneck-1.2.1 deepsurv-0.1.0 lasagne-0.1 lifelines-0.19.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHi-3XdPsL21",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lasagne\n",
        "import theano\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BQ_IWyHstOml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import deepsurv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dmhOuzQrhys",
        "colab_type": "code",
        "outputId": "499cce1e-927c-4687-ca0f-465a6aea13b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
        "!pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/Theano/Theano/archive/master.zip\n",
            "  Downloading https://github.com/Theano/Theano/archive/master.zip\n",
            "\u001b[K     \\ 23.4MB 80.7MB/s\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from Theano==1.0.4+unknown) (1.16.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from Theano==1.0.4+unknown) (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from Theano==1.0.4+unknown) (1.11.0)\n",
            "Building wheels for collected packages: Theano\n",
            "  Building wheel for Theano (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-L_qRDC/wheels/33/73/96/0ed263c62a86e2485ea634e0d3ae8169d50fd66e3b252541db\n",
            "Successfully built Theano\n",
            "Installing collected packages: Theano\n",
            "  Found existing installation: Theano 1.0.4+unknown\n",
            "    Uninstalling Theano-1.0.4+unknown:\n",
            "      Successfully uninstalled Theano-1.0.4+unknown\n",
            "Successfully installed Theano-1.0.4+unknown\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "theano"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/Lasagne/Lasagne/archive/master.zip\n",
            "  Downloading https://github.com/Lasagne/Lasagne/archive/master.zip\n",
            "\u001b[K     - 1.2MB 123.8MB/s\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python2.7/dist-packages (from Lasagne==0.2.dev1) (1.16.2)\n",
            "Building wheels for collected packages: Lasagne\n",
            "  Building wheel for Lasagne (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-tdt5Q0/wheels/ca/4a/00/87f1777b229481fe76562df7c0cfb993bc88ed0cc37e3f0ed4\n",
            "Successfully built Lasagne\n",
            "Installing collected packages: Lasagne\n",
            "  Found existing installation: Lasagne 0.1\n",
            "    Uninstalling Lasagne-0.1:\n",
            "      Successfully uninstalled Lasagne-0.1\n",
            "Successfully installed Lasagne-0.2.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ByIpaxw2rxbc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboard_logger\n",
        "import logging\n",
        "import tensorboard_logger \n",
        "from collections import defaultdict\n",
        "import sys\n",
        "import math\n",
        "\n",
        "class DeepSurvLogger():\n",
        "    def __init__(self, name):\n",
        "        self.logger         = logging.getLogger(name)\n",
        "        self.history = {}\n",
        "\n",
        "    def logMessage(self,message):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def print_progress_bar(self, step, max_steps, loss = None, ci = None, bar_length = 25, char = '*'):\n",
        "        progress_length = int(bar_length * step / max_steps)\n",
        "        progress_bar = [char] * (progress_length) + [' '] * (bar_length - progress_length)\n",
        "        space_padding = int(math.log10(max_steps))\n",
        "        if step > 0:\n",
        "            space_padding -= int(math.log10(step))\n",
        "        space_padding = ''.join([' '] * space_padding)\n",
        "        message = \"Training step %d/%d %s|\" % (step, max_steps, space_padding) + ''.join(progress_bar) + \"|\"\n",
        "        if loss:\n",
        "            message += \" - loss: %.4f\" % loss\n",
        "        if ci:\n",
        "            message += \" - ci: %.4f\" % ci\n",
        "\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def logValue(self, key, value, step):\n",
        "        pass\n",
        "\n",
        "    def shutdown(self):\n",
        "        logging.shutdown()\n",
        "\n",
        "class TensorboardLogger(DeepSurvLogger):\n",
        "    def __init__(self, name, logdir, max_steps = None, update_freq = 10):\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "        self.logger         = logging.getLogger(name)\n",
        "        self.logger.setLevel(logging.DEBUG)\n",
        "        ch = logging.StreamHandler(sys.stdout)\n",
        "        format = logging.Formatter(\"%(asctime)s - %(message)s\")\n",
        "        ch.setFormatter(format)\n",
        "        self.logger.addHandler(ch)\n",
        "\n",
        "        self.update_freq    = update_freq\n",
        "\n",
        "        self.tb_logger = tensorboard_logger.Logger(logdir)\n",
        "\n",
        "        self.history = defaultdict(list)\n",
        "\n",
        "    def logValue(self, key, value, step):\n",
        "        self.tb_logger.log_value(key, value, step)\n",
        "        self.history[key].append((step, value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RvKtHtjtAm2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import scipy.stats as st\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "import lasagne\n",
        "\n",
        "def load_datasets(dataset_file):\n",
        "    datasets = defaultdict(dict)\n",
        "\n",
        "    with h5py.File(dataset_file, 'r') as fp:\n",
        "        for ds in fp:\n",
        "            for array in fp[ds]:\n",
        "                datasets[ds][array] = fp[ds][array][:]\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def format_dataset_to_df(dataset, duration_col, event_col, trt_idx = None):\n",
        "    xdf = pd.DataFrame(dataset['x'])\n",
        "    if trt_idx is not None:\n",
        "        xdf = xdf.rename(columns={trt_idx : 'treat'})\n",
        "\n",
        "    dt = pd.DataFrame(dataset['t'], columns=[duration_col])\n",
        "    censor = pd.DataFrame(dataset['e'], columns=[event_col])\n",
        "    cdf = pd.concat([xdf, dt, censor], axis=1)\n",
        "    return cdf\n",
        "\n",
        "def standardize_dataset(dataset, offset, scale):\n",
        "    norm_ds = copy.deepcopy(dataset)\n",
        "    norm_ds['x'] = (norm_ds['x'] - offset) / scale\n",
        "    return norm_ds\n",
        "\n",
        "def bootstrap_metric(metric_fxn, dataset, N=100):\n",
        "    def sample_dataset(dataset, sample_idx):\n",
        "        sampled_dataset = {}\n",
        "        for (key,value) in dataset.items():\n",
        "            sampled_dataset[key] = value[sample_idx]\n",
        "        return sampled_dataset\n",
        "\n",
        "    metrics = []\n",
        "    size = len(dataset['x'])\n",
        "\n",
        "    for _ in range(N):\n",
        "        resample_idx = np.random.choice(size, size=size, replace = True)\n",
        "    \n",
        "        metric = metric_fxn(**sample_dataset(dataset, resample_idx))\n",
        "        metrics.append(metric)\n",
        "    \n",
        "    # Find mean and 95% confidence interval\n",
        "    mean = np.mean(metrics)\n",
        "    conf_interval = st.t.interval(0.95, len(metrics)-1, loc=mean, scale=st.sem(metrics))\n",
        "    return {\n",
        "        'mean': mean,\n",
        "        'confidence_interval': conf_interval\n",
        "    }\n",
        "\n",
        "def get_optimizer_from_str(update_fn):\n",
        "    if update_fn == 'sgd':\n",
        "        return lasagne.updates.sgd\n",
        "    elif update_fn == 'adam':\n",
        "        return lasagne.updates.adam\n",
        "    elif update_fn == 'rmsprop':\n",
        "        return lasagne.updates.rmsprop\n",
        "\n",
        "    return None\n",
        "\n",
        "def calculate_recs_and_antirecs(rec_trt, true_trt, dataset, print_metrics=True):\n",
        "    if isinstance(true_trt, int):\n",
        "        true_trt = dataset['x'][:,true_trt]\n",
        "\n",
        "    # trt_values = zip([0,1],np.sort(np.unique(true_trt)))\n",
        "    trt_values = enumerate(np.sort(np.unique(true_trt)))\n",
        "    equal_trt = [np.logical_and(rec_trt == rec_value, true_trt == true_value) for (rec_value, true_value) in trt_values]\n",
        "    rec_idx = np.logical_or(*equal_trt)\n",
        "    # original Logic\n",
        "    # rec_idx = np.logical_or(np.logical_and(rec_trt == 1,true_trt == 1),\n",
        "    #               np.logical_and(rec_trt == 0,true_trt == 0))\n",
        "\n",
        "    rec_t = dataset['t'][rec_idx]\n",
        "    antirec_t = dataset['t'][~rec_idx]\n",
        "    rec_e = dataset['e'][rec_idx]\n",
        "    antirec_e = dataset['e'][~rec_idx]\n",
        "\n",
        "    if print_metrics:\n",
        "        print(\"Printing treatment recommendation metrics\")\n",
        "        metrics = {\n",
        "            'rec_median' : np.median(rec_t),\n",
        "            'antirec_median' : np.median(antirec_t)\n",
        "        }\n",
        "        print(\"Recommendation metrics:\", metrics)\n",
        "\n",
        "    return {\n",
        "        'rec_t' : rec_t, \n",
        "        'rec_e' : rec_e, \n",
        "        'antirec_t' : antirec_t, \n",
        "        'antirec_e' : antirec_e\n",
        "    }\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O9Xq3-qFtFWN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install lifelines\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import pylab\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "from lifelines import KaplanMeierFitter\n",
        "from lifelines.statistics import logrank_test\n",
        "\n",
        "def extract_value_list(arr):\n",
        "    return list(np.array(arr)[:,1])\n",
        "\n",
        "def plot_log(log):\n",
        "    \"\"\"\n",
        "    Plots the training and validation curves for a network's loss function\n",
        "    and calculated concordance index.\n",
        "    Parameters:\n",
        "        log: a dictionary with a list of values for any of the following keys:\n",
        "            'train': training loss\n",
        "            'valid': validation loss\n",
        "            'train_ci': training concordance index\n",
        "            VALID_CI: validation concordance index\n",
        "    \"\"\"\n",
        "    TRAIN_LOSS = 'loss'\n",
        "    TRAIN_CI = 'c-index'\n",
        "    VALID_LOSS = 'valid_loss'\n",
        "    VALID_CI = 'valid_c-index'\n",
        "\n",
        "    num_epochs = len(log[TRAIN_LOSS])\n",
        "\n",
        "    # Plots Negative Log Likelihood vs. Epoch\n",
        "    fig, ax1 = plt.subplots()\n",
        "    # plt.figure()\n",
        "    handles = []\n",
        "    if TRAIN_LOSS in log:\n",
        "        epochs = range(num_epochs)\n",
        "        values = extract_value_list(log[TRAIN_LOSS])\n",
        "        train, = ax1.plot(epochs, values, 'b', label = 'Training')\n",
        "        ax1.tick_params('y', colors='b')\n",
        "        handles.append(train)\n",
        "    if VALID_LOSS in log:\n",
        "        ax2 = ax1.twinx()\n",
        "        epochs = np.linspace(0,num_epochs-1,num=len(log[VALID_LOSS]))\n",
        "        values = extract_value_list(log[VALID_LOSS])\n",
        "        valid, = ax2.plot(epochs,values, 'r', label = 'Validation')\n",
        "        ax2.tick_params('y', colors='r')\n",
        "        handles.append(valid)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Negative Log Likelihood')\n",
        "    plt.legend(handles=handles, loc = 0)\n",
        "\n",
        "    # Plots Concordance Index vs. Epoch\n",
        "    plt.figure()\n",
        "    handles = []\n",
        "    if TRAIN_CI in log:\n",
        "        epochs = np.linspace(0,num_epochs-1,num=len(log[TRAIN_CI]))\n",
        "        train, = plt.plot(epochs, extract_value_list(log[TRAIN_CI]), label = 'Training')\n",
        "        handles.append(train)\n",
        "    if VALID_CI in log:\n",
        "        epochs = np.linspace(0,num_epochs-1,num=len(log[VALID_CI]))\n",
        "        valid, = plt.plot(epochs, extract_value_list(log[VALID_CI]), label = 'Validation')\n",
        "        handles.append(valid)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Concordance Index')\n",
        "    plt.legend(handles = handles, loc = 4)\n",
        "\n",
        "def plot_risk_model(x_0, x_1, hr, figsize=(4,3), clim = (-3,3), cmap = 'jet'):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    plt.xlim(-1, 1)\n",
        "    plt.xlabel('$x_0$', fontsize='large')\n",
        "    plt.xticks(np.arange(-1, 1.5, .5))\n",
        "\n",
        "    plt.ylim(-1, 1)\n",
        "    plt.ylabel('$x_1$', fontsize='large')\n",
        "    plt.yticks(np.arange(-1, 1.5, .5))\n",
        "    \n",
        "    im = plt.scatter(x=x_0, y=x_1, c=hr, marker='.', cmap=cmap)\n",
        "    fig.colorbar(im)\n",
        "    # plt.clim(0, 1)\n",
        "    plt.clim(*clim)\n",
        "    plt.tight_layout()\n",
        "    return (fig, ax, im)\n",
        "\n",
        "def save_fig(fig, fp):\n",
        "    # TODO fit the pdf saving cutting off the x and y axis labels\n",
        "    pp_true = PdfPages(fp)\n",
        "    pp_true.savefig(fig, dpi=600)\n",
        "    pp_true.close()\n",
        "\n",
        "def plot_experiment_scatters(risk_fxn, dataset, norm_vals = None, output_file=None, \n",
        "    figsize = (4,3), clim=(-3,3), cmap = 'jet', plot_error=False, trt_idx = None):\n",
        "    \n",
        "    def norm_hr(hr):\n",
        "        # return hr\n",
        "        return hr - hr.mean();\n",
        "        # return (hr - hr.min()) / (hr.max() - hr.min())\n",
        "\n",
        "    x_0 = dataset['x'][:, 0]\n",
        "    x_1 = dataset['x'][:, 1]\n",
        "\n",
        "    # Plot model predictions\n",
        "    x = dataset['x']\n",
        "    if norm_vals:\n",
        "        x = (x - norm_vals['mean']) / norm_vals['std']\n",
        "\n",
        "    (head, tail) = os.path.split(output_file)\n",
        "\n",
        "    if not trt_idx is None:\n",
        "        trt_values = np.unique(x[:,trt_idx])\n",
        "        for (idx,trt_value) in enumerate(trt_values):\n",
        "            x_trt = np.copy(x)\n",
        "            x_trt[:,trt_idx] = trt_value\n",
        "            hr_trt = risk_fxn(x_trt)\n",
        "            hr_trt = norm_hr(hr_trt)\n",
        "            fig_trt, _, _ = plot_risk_model(x_0, x_1, hr_trt, figsize, clim, cmap)\n",
        "\n",
        "            if output_file:\n",
        "                save_fig(fig_trt, os.path.join(head, \"treatment_%d_\" % idx + tail))\n",
        "    else:\n",
        "        hr_pred = risk_fxn(x)\n",
        "        hr_pred = norm_hr(hr_pred)\n",
        "        fig_pred, _, _ = plot_risk_model(x_0, x_1, hr_pred, figsize, clim, cmap)\n",
        "\n",
        "        if output_file:\n",
        "            save_fig(fig_pred, os.path.join(head, \"pred_\" + tail))\n",
        "\n",
        "    if 'hr' in dataset:\n",
        "        hr_true = dataset['hr']\n",
        "        hr_true = norm_hr(hr_true)\n",
        "        fig_true, _, _ = plot_risk_model(x_0, x_1, hr_true, figsize, clim, cmap)\n",
        "\n",
        "        if output_file:\n",
        "            save_fig(fig_true, os.path.join(head, \"true_\" + tail))\n",
        "\n",
        "        if plot_error:\n",
        "            hr_error = np.abs(hr_true - hr_pred)\n",
        "            fig_error, _, _ = plot_risk_model(x_0, x_1, hr_error, figsize, clim=(0,20), cmap = cmap)\n",
        "\n",
        "            if output_file:\n",
        "                save_fig(fig_error, os.path.join(head, \"error_\" + tail))\n",
        "\n",
        "def plot_survival_curves(rec_t, rec_e, antirec_t, antirec_e, experiment_name = '', output_file = None):\n",
        "    # Set-up plots\n",
        "    plt.figure(figsize=(12,3))\n",
        "    ax = plt.subplot(111)\n",
        "\n",
        "    # Fit survival curves\n",
        "    kmf = KaplanMeierFitter()\n",
        "    kmf.fit(rec_t, event_observed=rec_e, label=' '.join([experiment_name, \"Recommendation\"]))   \n",
        "    kmf.plot(ax=ax,linestyle=\"-\")\n",
        "    kmf.fit(antirec_t, event_observed=antirec_e, label=' '.join([experiment_name, \"Anti-Recommendation\"]))\n",
        "    kmf.plot(ax=ax,linestyle=\"--\")\n",
        "    \n",
        "    # Format graph\n",
        "    plt.ylim(0,1);\n",
        "    ax.set_xlabel('Timeline (months)',fontsize='large')\n",
        "    ax.set_ylabel('Percentage of Population Alive',fontsize='large')\n",
        "    \n",
        "    # Calculate p-value\n",
        "    results = logrank_test(rec_t, antirec_t, rec_e, antirec_e, alpha=.95)\n",
        "    results.print_summary()\n",
        "\n",
        "    # Location the label at the 1st out of 9 tick marks\n",
        "    xloc = max(np.max(rec_t),np.max(antirec_t)) / 9\n",
        "    if results.p_value < 1e-5:\n",
        "        ax.text(xloc,.2,'$p < 1\\mathrm{e}{-5}$',fontsize=20)\n",
        "    else:\n",
        "        ax.text(xloc,.2,'$p=%f$' % results.p_value,fontsize=20)\n",
        "    plt.legend(loc='best',prop={'size':15})\n",
        "\n",
        "\n",
        "    if output_file:\n",
        "        plt.tight_layout()\n",
        "        pylab.savefig(output_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HD5BFbqSCPlf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, absolute_import\n",
        "\n",
        "import lasagne\n",
        "import numpy\n",
        "import time\n",
        "import json\n",
        "import h5py\n",
        "\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "\n",
        "from lifelines.utils import concordance_index\n",
        "\n",
        "#from .deepsurv_logger import DeepSurvLogger\n",
        "\n",
        "from lasagne.regularization import regularize_layer_params, l1, l2\n",
        "from lasagne.nonlinearities import rectify,selu\n",
        "\n",
        "class DeepSurv:\n",
        "    def __init__(self, n_in,\n",
        "    learning_rate, hidden_layers_sizes = None,\n",
        "    lr_decay = 0.0, momentum = 0.9,\n",
        "    L2_reg = 0.0, L1_reg = 0.0,\n",
        "    activation = \"rectify\",\n",
        "    dropout = None,\n",
        "    batch_norm = False,\n",
        "    standardize = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This class implements and trains a DeepSurv model.\n",
        "        Parameters:\n",
        "            n_in: number of input nodes.\n",
        "            learning_rate: learning rate for training.\n",
        "            lr_decay: coefficient for Power learning rate decay.\n",
        "            L2_reg: coefficient for L2 weight decay regularization. Used to help\n",
        "                prevent the model from overfitting.\n",
        "            L1_reg: coefficient for L1 weight decay regularization\n",
        "            momentum: coefficient for momentum. Can be 0 or None to disable.\n",
        "            hidden_layer_sizes: a list of integers to determine the size of\n",
        "                each hidden layer.\n",
        "            activation: a lasagne activation class.\n",
        "                Default: lasagne.nonlinearities.rectify\n",
        "            batch_norm: True or False. Include batch normalization layers.\n",
        "            dropout: if not None or 0, the percentage of dropout to include\n",
        "                after each hidden layer. Default: None\n",
        "            standardize: True or False. Include standardization layer after\n",
        "                input layer.\n",
        "        \"\"\"\n",
        "\n",
        "        self.X = T.fmatrix('x')  # patients covariates\n",
        "        self.E = T.ivector('e') # the observations vector\n",
        "\n",
        "        # Default Standardization Values: mean = 0, std = 1\n",
        "        self.offset = numpy.zeros(shape = n_in, dtype=numpy.float32)\n",
        "        self.scale = numpy.ones(shape = n_in, dtype=numpy.float32)\n",
        "\n",
        "        # self.offset = theano.shared(numpy.zeros(shape = n_in, dtype=numpy.float32))\n",
        "        # self.scale = theano.shared(numpy.ones(shape = n_in, dtype=numpy.float32))\n",
        "\n",
        "        network = lasagne.layers.InputLayer(shape=(None,n_in),\n",
        "            input_var = self.X)\n",
        "\n",
        "        # if standardize:\n",
        "        #     network = lasagne.layers.standardize(network,self.offset,\n",
        "        #                                         self.scale,\n",
        "        #                                         shared_axes = 0)\n",
        "        self.standardize = standardize\n",
        "\n",
        "        if activation == 'rectify':\n",
        "            activation_fn = rectify\n",
        "        elif activation == 'selu':\n",
        "            activation_fn = selu\n",
        "        else:\n",
        "            raise IllegalArgumentException(\"Unknown activation function: %s\" % activation)\n",
        "\n",
        "        # Construct Neural Network\n",
        "        for n_layer in (hidden_layers_sizes or []):\n",
        "            if activation_fn == lasagne.nonlinearities.rectify:\n",
        "                W_init = lasagne.init.GlorotUniform()\n",
        "            else:\n",
        "                # TODO: implement other initializations\n",
        "                W_init = lasagne.init.GlorotUniform()\n",
        "\n",
        "            network = lasagne.layers.DenseLayer(\n",
        "                network, num_units = n_layer,\n",
        "                nonlinearity = activation_fn,\n",
        "                W = W_init\n",
        "            )\n",
        "\n",
        "            if batch_norm:\n",
        "                network = lasagne.layers.batch_norm(network)\n",
        "\n",
        "            if not dropout is None:\n",
        "                network = lasagne.layers.DropoutLayer(network, p = dropout)\n",
        "\n",
        "        # Combine Linear to output Log Hazard Ratio - same as Faraggi\n",
        "        network = lasagne.layers.DenseLayer(\n",
        "            network, num_units = 1,\n",
        "            nonlinearity = lasagne.nonlinearities.linear,\n",
        "            W = lasagne.init.GlorotUniform()\n",
        "        )\n",
        "\n",
        "        self.network = network\n",
        "        self.params = lasagne.layers.get_all_params(self.network,\n",
        "                                                    trainable = True)\n",
        "        self.hidden_layers = lasagne.layers.get_all_layers(self.network)[1:]\n",
        "\n",
        "        # Relevant Functions\n",
        "        self.partial_hazard = T.exp(self.risk(deterministic = True)) # e^h(x)\n",
        "\n",
        "        # Store and set needed Hyper-parameters:\n",
        "        self.hyperparams = {\n",
        "            'n_in': n_in,\n",
        "            'learning_rate': learning_rate,\n",
        "            'hidden_layers_sizes': hidden_layers_sizes,\n",
        "            'lr_decay': lr_decay,\n",
        "            'momentum': momentum,\n",
        "            'L2_reg': L2_reg,\n",
        "            'L1_reg': L1_reg,\n",
        "            'activation': activation,\n",
        "            'dropout': dropout,\n",
        "            'batch_norm': batch_norm,\n",
        "            'standardize': standardize\n",
        "        }\n",
        "\n",
        "        self.n_in = n_in\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lr_decay = lr_decay\n",
        "        self.L2_reg = L2_reg\n",
        "        self.L1_reg = L1_reg\n",
        "        self.momentum = momentum\n",
        "        self.restored_update_params = None\n",
        "\n",
        "    def _negative_log_likelihood(self, E, deterministic = False):\n",
        "        \"\"\"Return the negative average log-likelihood of the prediction\n",
        "            of this model under a given target distribution.\n",
        "        .. math::\n",
        "            \\frac{1}{N_D} \\sum_{i \\in D}[F(x_i,\\theta) - log(\\sum_{j \\in R_i} e^F(x_j,\\theta))]\n",
        "                - \\lambda P(\\theta)\n",
        "        where:\n",
        "            D is the set of observed events\n",
        "            N_D is the number of observed events\n",
        "            R_i is the set of examples that are still alive at time of death t_j\n",
        "            F(x,\\theta) = log hazard rate\n",
        "        Note: We assume that there are no tied event times\n",
        "        Parameters:\n",
        "            E (n,): TensorVector that corresponds to a vector that gives the censor\n",
        "                variable for each example\n",
        "            deterministic: True or False. Determines if the output of the network\n",
        "                is calculated determinsitically.\n",
        "        Returns:\n",
        "            neg_likelihood: Theano expression that computes negative\n",
        "                partial Cox likelihood\n",
        "        \"\"\"\n",
        "        risk = self.risk(deterministic)\n",
        "        hazard_ratio = T.exp(risk)\n",
        "        log_risk = T.log(T.extra_ops.cumsum(hazard_ratio))\n",
        "        uncensored_likelihood = risk.T - log_risk\n",
        "        censored_likelihood = uncensored_likelihood * E\n",
        "        num_observed_events = T.sum(E)\n",
        "        neg_likelihood = -T.sum(censored_likelihood) / num_observed_events\n",
        "        return neg_likelihood\n",
        "\n",
        "    def _get_loss_updates(self,\n",
        "    L1_reg = 0.0, L2_reg = 0.001,\n",
        "    update_fn = lasagne.updates.nesterov_momentum,\n",
        "    max_norm = None, deterministic = False,\n",
        "    momentum = 0.9,\n",
        "    **kwargs):\n",
        "        \"\"\"\n",
        "        Returns Theano expressions for the network's loss function and parameter\n",
        "            updates.\n",
        "        Parameters:\n",
        "            L1_reg: float for L1 weight regularization coefficient.\n",
        "            L2_reg: float for L2 weight regularization coefficient.\n",
        "            max_norm: If not None, constraints the norm of gradients to be less\n",
        "                than max_norm.\n",
        "            deterministic: True or False. Determines if the output of the network\n",
        "                is calculated determinsitically.\n",
        "            update_fn: lasagne update function.\n",
        "                Default: Stochastic Gradient Descent with Nesterov momentum\n",
        "            **kwargs: additional parameters to provide to update_fn.\n",
        "                For example: momentum\n",
        "        Returns:\n",
        "            loss: Theano expression for a penalized negative log likelihood.\n",
        "            updates: Theano expression to update the parameters using update_fn.\n",
        "        \"\"\"\n",
        "\n",
        "        loss = (\n",
        "            self._negative_log_likelihood(self.E, deterministic)\n",
        "            + regularize_layer_params(self.network,l1) * L1_reg\n",
        "            + regularize_layer_params(self.network, l2) * L2_reg\n",
        "        )\n",
        "\n",
        "        if max_norm:\n",
        "            grads = T.grad(loss,self.params)\n",
        "            scaled_grads = lasagne.updates.total_norm_constraint(grads, max_norm)\n",
        "            updates = update_fn(\n",
        "                scaled_grads, self.params, **kwargs\n",
        "            )\n",
        "        else:\n",
        "            updates = update_fn(\n",
        "                loss, self.params, **kwargs\n",
        "            )\n",
        "\n",
        "        if momentum:\n",
        "            updates = lasagne.updates.apply_nesterov_momentum(updates, \n",
        "                self.params, self.learning_rate, momentum=momentum)\n",
        "\n",
        "        # If the model was loaded from file, reload params\n",
        "        if self.restored_update_params:\n",
        "            for p, value in zip(updates.keys(), self.restored_update_params):\n",
        "                p.set_value(value)\n",
        "            self.restored_update_params = None\n",
        "\n",
        "        # Store last update function to be later saved\n",
        "        self.updates = updates\n",
        "\n",
        "        return loss, updates\n",
        "\n",
        "    def _get_train_valid_fn(self,\n",
        "    L1_reg, L2_reg, learning_rate,\n",
        "    **kwargs):\n",
        "        \"\"\"\n",
        "        Builds the loss and update Theano expressions into callable Theano functions.\n",
        "        Parameters:\n",
        "            L1_reg: coefficient for L1 weight decay regularization\n",
        "            L2_reg: coefficient for L2 weight decay regularization. Used to help\n",
        "                prevent the model from overfitting.\n",
        "            learning_rate: learning rate coefficient.\n",
        "            **kwargs: additional parameters to provide to _get_loss_updates.\n",
        "        Returns:\n",
        "            train_fn: Theano function that takes a (n, d) array and (n,) vector\n",
        "                and computes the loss function and updates the network parameters.\n",
        "                Calculated non-deterministically.\n",
        "            valid_fn: Theano function that takes a (n, d) array and (n,) vector\n",
        "                and computes the loss function without updating the network parameters.\n",
        "                Calcualted deterministically.\n",
        "        \"\"\"\n",
        "\n",
        "        loss, updates = self._get_loss_updates(\n",
        "            L1_reg, L2_reg, deterministic = False,\n",
        "            learning_rate=learning_rate, **kwargs\n",
        "        )\n",
        "        train_fn = theano.function(\n",
        "            inputs = [self.X, self.E],\n",
        "            outputs = loss,\n",
        "            updates = updates,\n",
        "            name = 'train'\n",
        "        )\n",
        "\n",
        "        valid_loss, _ = self._get_loss_updates(\n",
        "            L1_reg, L2_reg, deterministic = True,\n",
        "            learning_rate=learning_rate, **kwargs\n",
        "        )\n",
        "\n",
        "        valid_fn = theano.function(\n",
        "            inputs = [self.X, self.E],\n",
        "            outputs = valid_loss,\n",
        "            name = 'valid'\n",
        "        )\n",
        "        return train_fn, valid_fn\n",
        "\n",
        "    def get_concordance_index(self, x, t, e, **kwargs):\n",
        "        \"\"\"\n",
        "        Taken from the lifelines.utils package. Docstring is provided below.\n",
        "        Parameters:\n",
        "            x: (n, d) numpy array of observations.\n",
        "            t: (n) numpy array representing observed time events.\n",
        "            e: (n) numpy array representing time indicators.\n",
        "        Returns:\n",
        "            concordance_index: calcualted using lifelines.utils.concordance_index\n",
        "        lifelines.utils.concordance index docstring:\n",
        "        Calculates the concordance index (C-index) between two series\n",
        "        of event times. The first is the real survival times from\n",
        "        the experimental data, and the other is the predicted survival\n",
        "        times from a model of some kind.\n",
        "        The concordance index is a value between 0 and 1 where,\n",
        "        0.5 is the expected result from random predictions,\n",
        "        1.0 is perfect concordance and,\n",
        "        0.0 is perfect anti-concordance (multiply predictions with -1 to get 1.0)\n",
        "        Score is usually 0.6-0.7 for survival models.\n",
        "        See:\n",
        "        Harrell FE, Lee KL, Mark DB. Multivariable prognostic models: issues in\n",
        "        developing models, evaluating assumptions and adequacy, and measuring and\n",
        "        reducing errors. Statistics in Medicine 1996;15(4):361-87.\n",
        "        \"\"\"\n",
        "        compute_hazards = theano.function(\n",
        "            inputs = [self.X],\n",
        "            outputs = -self.partial_hazard\n",
        "        )\n",
        "        partial_hazards = compute_hazards(x)\n",
        "\n",
        "        return concordance_index(t,\n",
        "            partial_hazards,\n",
        "            e)\n",
        "\n",
        "    def _standardize_x(self, x):\n",
        "        return (x - self.offset) / self.scale\n",
        "\n",
        "    # @TODO: implement for varios instances of datasets\n",
        "    def prepare_data(self,dataset):\n",
        "        if isinstance(dataset, dict):\n",
        "            x, e, t = dataset['x'], dataset['e'], dataset['t']\n",
        "\n",
        "        if self.standardize:\n",
        "            x = self._standardize_x(x)\n",
        "\n",
        "        # Sort Training Data for Accurate Likelihood\n",
        "        sort_idx = numpy.argsort(t)[::-1]\n",
        "        x = x[sort_idx]\n",
        "        e = e[sort_idx]\n",
        "        t = t[sort_idx]\n",
        "\n",
        "        return (x, e, t)\n",
        "\n",
        "    def train(self,\n",
        "    train_data, valid_data= None,\n",
        "    n_epochs = 500,\n",
        "    validation_frequency = 250,\n",
        "    patience = 2000, improvement_threshold = 0.99999, patience_increase = 2,\n",
        "    logger = None,\n",
        "    update_fn = lasagne.updates.nesterov_momentum,\n",
        "    verbose = True,\n",
        "    **kwargs):\n",
        "        \"\"\"\n",
        "        Trains a DeepSurv network on the provided training data and evalutes\n",
        "            it on the validation data.\n",
        "        Parameters:\n",
        "            train_data: dictionary with the following keys:\n",
        "                'x' : (n,d) array of observations (dtype = float32).\n",
        "                't' : (n) array of observed time events (dtype = float32).\n",
        "                'e' : (n) array of observed time indicators (dtype = int32).\n",
        "            valid_data: optional. A dictionary with the following keys:\n",
        "                'x' : (n,d) array of observations.\n",
        "                't' : (n) array of observed time events.\n",
        "                'e' : (n) array of observed time indicators.\n",
        "            standardize: True or False. Set the offset and scale of\n",
        "                standardization layey to the mean and standard deviation of the\n",
        "                training data.\n",
        "            n_epochs: integer for the maximum number of epochs the network will\n",
        "                train for.\n",
        "            validation_frequency: how often the network computes the validation\n",
        "                metrics. Decreasing validation_frequency increases training speed.\n",
        "            patience: minimum number of epochs to train for. Once patience is\n",
        "                reached, looks at validation improvement to increase patience or\n",
        "                early stop.\n",
        "            improvement_threshold: percentage of improvement needed to increase\n",
        "                patience.\n",
        "            patience_increase: multiplier to patience if threshold is reached.\n",
        "            logger: None or DeepSurvLogger.\n",
        "            update_fn: lasagne update function for training.\n",
        "                Default: lasagne.updates.nesterov_momentum\n",
        "            **kwargs: additional parameters to provide _get_train_valid_fn.\n",
        "                Parameters used to provide configurations to update_fn.\n",
        "        Returns:\n",
        "            metrics: a dictionary of training metrics that include:\n",
        "                'train': a list of loss values for each training epoch\n",
        "                'train_ci': a list of C-indices for each training epoch\n",
        "                'best_params': a list of numpy arrays containing the parameters\n",
        "                    when the network had the best validation loss\n",
        "                'best_params_idx': the epoch at which best_params was found\n",
        "            If valid_data is provided, the metrics also contain:\n",
        "                'valid': a list of validation loss values for each validation frequency\n",
        "                'valid_ci': a list of validation C-indiices for each validation frequency\n",
        "                'best_validation_loss': the best validation loss found during training\n",
        "                'best_valid_ci': the max validation C-index found during training\n",
        "        \"\"\"\n",
        "        if logger is None:\n",
        "            logger = DeepSurvLogger('DeepSurv')\n",
        "\n",
        "        # Set Standardization layer offset and scale to training data mean and std\n",
        "        if self.standardize:\n",
        "            self.offset = train_data['x'].mean(axis = 0)\n",
        "            self.scale = train_data['x'].std(axis = 0)\n",
        "\n",
        "        x_train, e_train, t_train = self.prepare_data(train_data)\n",
        "\n",
        "        if valid_data:\n",
        "            x_valid, e_valid, t_valid = self.prepare_data(valid_data)\n",
        "\n",
        "        # Initialize Metrics\n",
        "        best_validation_loss = numpy.inf\n",
        "        best_params = None\n",
        "        best_params_idx = -1\n",
        "\n",
        "        # Initialize Training Parameters\n",
        "        lr = theano.shared(numpy.array(self.learning_rate,\n",
        "                                    dtype = numpy.float32))\n",
        "        momentum = numpy.array(0, dtype= numpy.float32)\n",
        "\n",
        "        train_fn, valid_fn = self._get_train_valid_fn(\n",
        "            L1_reg=self.L1_reg, L2_reg=self.L2_reg,\n",
        "            learning_rate=lr,\n",
        "            momentum = momentum,\n",
        "            update_fn = update_fn, **kwargs\n",
        "        )\n",
        "\n",
        "        start = time.time()\n",
        "        for epoch in range(n_epochs):\n",
        "            # Power-Learning Rate Decay\n",
        "            lr = self.learning_rate / (1 + epoch * self.lr_decay)\n",
        "            logger.logValue('lr', lr, epoch)\n",
        "\n",
        "            if self.momentum and epoch >= 10:\n",
        "                momentum = self.momentum\n",
        "\n",
        "            loss = train_fn(x_train, e_train)\n",
        "\n",
        "            logger.logValue('loss', loss, epoch)\n",
        "            # train_loss.append(loss)\n",
        "\n",
        "            ci_train = self.get_concordance_index(\n",
        "                x_train,\n",
        "                t_train,\n",
        "                e_train,\n",
        "            )\n",
        "            logger.logValue('c-index',ci_train, epoch)\n",
        "            # train_ci.append(ci_train)\n",
        "\n",
        "            if valid_data and (epoch % validation_frequency == 0):\n",
        "                validation_loss = valid_fn(x_valid, e_valid)\n",
        "                logger.logValue('valid_loss',validation_loss, epoch)\n",
        "\n",
        "                ci_valid = self.get_concordance_index(\n",
        "                    x_valid,\n",
        "                    t_valid,\n",
        "                    e_valid\n",
        "                )\n",
        "                logger.logValue('valid_c-index', ci_valid, epoch)\n",
        "\n",
        "                if validation_loss < best_validation_loss:\n",
        "                    # improve patience if loss improves enough\n",
        "                    if validation_loss < best_validation_loss * improvement_threshold:\n",
        "                        patience = max(patience, epoch * patience_increase)\n",
        "\n",
        "                    best_params = [param.copy().eval() for param in self.params]\n",
        "                    best_params_idx = epoch\n",
        "                    best_validation_loss = validation_loss\n",
        "\n",
        "            if verbose and (epoch % validation_frequency == 0):\n",
        "                logger.print_progress_bar(epoch, n_epochs, loss, ci_train)\n",
        "\n",
        "            if patience <= epoch:\n",
        "                break\n",
        "\n",
        "        if verbose:\n",
        "            logger.logMessage('Finished Training with %d iterations in %0.2fs' % (\n",
        "                epoch + 1, time.time() - start\n",
        "            ))\n",
        "        logger.shutdown()\n",
        "\n",
        "        # Return Logger.getMetrics()\n",
        "        # metrics = {\n",
        "        #     'train': train_loss,\n",
        "        #     'best_params': best_params,\n",
        "        #     'best_params_idx' : best_params_idx,\n",
        "        #     'train_ci' : train_ci\n",
        "        # }\n",
        "        # if valid_data:\n",
        "        #     metrics.update({\n",
        "        #         'valid' : valid_loss,\n",
        "        #         'valid_ci': valid_ci,\n",
        "        #         'best_valid_ci': max(valid_ci),\n",
        "        #         'best_validation_loss':best_validation_loss\n",
        "        #     })\n",
        "        logger.history['best_valid_loss'] = best_validation_loss\n",
        "        logger.history['best_params'] = best_params\n",
        "        logger.history['best_params_idx'] = best_params_idx\n",
        "\n",
        "        return logger.history\n",
        "\n",
        "    def to_json(self):\n",
        "        return json.dumps(self.hyperparams)\n",
        "\n",
        "    def save_model(self, filename, weights_file = None):\n",
        "        with open(filename, 'w') as fp:\n",
        "            fp.write(self.to_json())\n",
        "\n",
        "        if weights_file:\n",
        "            self.save_weights(weights_file)\n",
        "\n",
        "    def save_weights(self,filename):\n",
        "        def save_list_by_idx(group, lst):\n",
        "            for (idx, param) in enumerate(lst):\n",
        "                group.create_dataset(str(idx), data=param)\n",
        "\n",
        "        weights_out = lasagne.layers.get_all_param_values(self.network, trainable=False)\n",
        "        if self.updates:\n",
        "            updates_out = [p.get_value() for p in self.updates.keys()]\n",
        "        else:\n",
        "            raise Exception(\"Model has not been trained: no params to save!\")\n",
        "\n",
        "        # Store all of the parameters in an hd5f file\n",
        "        # We store the parameter under the index in the list\n",
        "        # so that when we read it later, we can construct the list of\n",
        "        # parameters in the same order they were saved\n",
        "        with h5py.File(filename, 'w') as f_out:\n",
        "            weights_grp = f_out.create_group('weights')\n",
        "            save_list_by_idx(weights_grp, weights_out)\n",
        "\n",
        "            updates_grp = f_out.create_group('updates')\n",
        "            save_list_by_idx(updates_grp, updates_out)\n",
        "\n",
        "    def load_weights(self, filename):\n",
        "        def load_all_keys(fp):\n",
        "            results = []\n",
        "            for key in fp:\n",
        "                dataset = fp[key][:]\n",
        "                results.append((int(key), dataset))\n",
        "            return results\n",
        "\n",
        "        def sort_params_by_idx(params):\n",
        "            return [param for (idx, param) in sorted(params, \n",
        "            key=lambda param: param[0])]\n",
        "\n",
        "        # Load all of the parameters\n",
        "        with h5py.File(filename, 'r') as f_in:\n",
        "            weights_in = load_all_keys(f_in['weights'])\n",
        "            updates_in = load_all_keys(f_in['updates'])\n",
        "\n",
        "        # Sort them according to the idx to ensure they are set correctly\n",
        "        sorted_weights_in = sort_params_by_idx(weights_in)\n",
        "        lasagne.layers.set_all_param_values(self.network, sorted_weights_in, \n",
        "            trainable=False)\n",
        "\n",
        "        sorted_updates_in = sort_params_by_idx(updates_in)\n",
        "        self.restored_update_params = sorted_updates_in\n",
        "\n",
        "    def risk(self,deterministic = False):\n",
        "        \"\"\"\n",
        "        Returns a theano expression for the output of network which is an\n",
        "            observation's predicted risk.\n",
        "        Parameters:\n",
        "            deterministic: True or False. Determines if the output of the network\n",
        "                is calculated determinsitically.\n",
        "        Returns:\n",
        "            risk: a theano expression representing a predicted risk h(x)\n",
        "        \"\"\"\n",
        "        return lasagne.layers.get_output(self.network,\n",
        "                                        deterministic = deterministic)\n",
        "\n",
        "    def predict_risk(self, x):\n",
        "        \"\"\"\n",
        "        Calculates the predicted risk for an array of observations.\n",
        "        Parameters:\n",
        "            x: (n,d) numpy array of observations.\n",
        "        Returns:\n",
        "            risks: (n) array of predicted risks\n",
        "        \"\"\"\n",
        "        risk_fxn = theano.function(\n",
        "            inputs = [self.X],\n",
        "            outputs = self.risk(deterministic= True),\n",
        "            name = 'predicted risk'\n",
        "        )\n",
        "        return risk_fxn(x)\n",
        "\n",
        "    def recommend_treatment(self, x, trt_i, trt_j, trt_idx = -1):\n",
        "        \"\"\"\n",
        "        Computes recommendation function rec_ij(x) for two treatments i and j.\n",
        "            rec_ij(x) is the log of the hazards ratio of x in treatment i vs.\n",
        "            treatment j.\n",
        "        .. math::\n",
        "            rec_{ij}(x) = log(e^h_i(x) / e^h_j(x)) = h_i(x) - h_j(x)\n",
        "        Parameters:\n",
        "            x: (n, d) numpy array of observations\n",
        "            trt_i: treatment i value\n",
        "            trt_j: treatment j value\n",
        "            trt_idx: the index of x representing the treatment group column\n",
        "        Returns:\n",
        "            rec_ij: recommendation\n",
        "        \"\"\"\n",
        "        # Copy x to prevent overwritting data\n",
        "        x_trt = numpy.copy(x)\n",
        "\n",
        "        # Calculate risk of observations treatment i\n",
        "        x_trt[:,trt_idx] = trt_i\n",
        "        h_i = self.predict_risk(x_trt)\n",
        "        # Risk of observations in treatment j\n",
        "        x_trt[:,trt_idx] = trt_j;\n",
        "        h_j = self.predict_risk(x_trt)\n",
        "\n",
        "        rec_ij = h_i - h_j\n",
        "        return rec_ij\n",
        "\n",
        "    def plot_risk_surface(self, data, i = 0, j = 1,\n",
        "        figsize = (6,4), x_lims = None, y_lims = None, c_lims = None):\n",
        "        \"\"\"\n",
        "        Plots the predicted risk surface of the network with respect to two\n",
        "        observed covarites i and j.\n",
        "        Parameters:\n",
        "            data: (n,d) numpy array of observations of which to predict risk.\n",
        "            i: index of data to plot as axis 1\n",
        "            j: index of data to plot as axis 2\n",
        "            figsize: size of figure for matplotlib\n",
        "            x_lims: Optional. If provided, override default x_lims (min(x_i), max(x_i))\n",
        "            y_lims: Optional. If provided, override default y_lims (min(x_j), max(x_j))\n",
        "            c_lims: Optional. If provided, override default color limits.\n",
        "        Returns:\n",
        "            fig: matplotlib figure object.\n",
        "        \"\"\"\n",
        "        fig = plt.figure(figsize=figsize)\n",
        "        X = data[:,i]\n",
        "        Y = data[:,j]\n",
        "        Z = self.predict_risk(data)\n",
        "\n",
        "        if not x_lims is None:\n",
        "            x_lims = [np.round(np.min(X)), np.round(np.max(X))]\n",
        "        if not y_lims is None:\n",
        "            y_lims = [np.round(np.min(Y)), np.round(np.max(Y))]\n",
        "        if not c_lims is None:\n",
        "            c_lims = [np.round(np.min(Z)), np.round(np.max(Z))]\n",
        "\n",
        "        ax = plt.scatter(X,Y, c = Z, edgecolors = 'none', marker = '.')\n",
        "        ax.set_clim(*c_lims)\n",
        "        plt.colorbar()\n",
        "        plt.xlim(*x_lims)\n",
        "        plt.ylim(*y_lims)\n",
        "        plt.xlabel('$x_{%d}$' % i, fontsize=18)\n",
        "        plt.ylabel('$x_{%d}$' % j, fontsize=18)\n",
        "\n",
        "        return fig\n",
        "\n",
        "def load_model_from_json(model_fp, weights_fp = None):\n",
        "    with open(model_fp, 'r') as fp:\n",
        "        json_model = fp.read()\n",
        "    print('Loading json model:',json_model)\n",
        "    hyperparams = json.loads(json_model)\n",
        "\n",
        "    model = DeepSurv(**hyperparams)\n",
        "\n",
        "    if weights_fp:\n",
        "        model.load_weights(weights_fp)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qxd71mLftRwF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2VYwgw_SKY-X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B_BRYDVt7OVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "link = 'https://drive.google.com/open?id=1TWOQRK0esxxu5l97JhGwfrGCzjHTcBNl' # The shareable link"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lLN-3uU7KnD9",
        "colab_type": "code",
        "outputId": "f0af5544-9df9-4897-d10c-8fc1a07ff286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "fluff, id = link.split('=')\n",
        "print (id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1TWOQRK0esxxu5l97JhGwfrGCzjHTcBNl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "71dWgSwXKop2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('va_train.csv')  \n",
        "df3_train = pd.read_csv('va_train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNlVKG7z80nX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dataframe_to_deepsurv_ds(df, event_col = 'status', time_col = 'stime'):\n",
        "    # Extract the event and time columns as numpy arrays\n",
        "    e = df[event_col].values.astype(np.int32)\n",
        "    t = df[time_col].values.astype(np.float32)\n",
        "\n",
        "    # Extract the patient's covariates as a numpy array\n",
        "    x_df = df.drop([event_col, time_col], axis = 1)\n",
        "    x = x_df.values.astype(np.float32)\n",
        "    \n",
        "    # Return the deep surv dataframe\n",
        "    return {\n",
        "        'x' : x,\n",
        "        'e' : e,\n",
        "        't' : t\n",
        "    }\n",
        "\n",
        "# If the headers of the csv change, you can replace the values of \n",
        "# 'event_col' and 'time_col' with the names of the new headers\n",
        "# You can also use this function on your training dataset, validation dataset, and testing dataset\n",
        "train_data = dataframe_to_deepsurv_ds(df3_train, event_col = 'status', time_col= 'stime')\n",
        "valid_data = dataframe_to_deepsurv_ds(df3_valid, event_col = 'status', time_col= 'stime')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ub_OUV-R9bVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hyperparams = {\n",
        "    'L2_reg': 10.0,\n",
        "    'batch_norm': True,\n",
        "    'dropout': 0.4,\n",
        "    'hidden_layers_sizes': [25, 25],\n",
        "    'learning_rate': 1e-05,\n",
        "    'lr_decay': 0.001,\n",
        "    'momentum': 0.9,\n",
        "    'n_in': train_data['x'].shape[1],\n",
        "    'standardize': True\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vd0ENUcM9eOd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = deepsurv.DeepSurv(**hyperparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edOBA0x6ZThd",
        "colab_type": "code",
        "outputId": "dd53f5ef-2999-4428-d831-6dc81b349c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "logger= None \n",
        "update_fn=lasagne.updates.nesterov_momentum \n",
        "n_epochs = 2000\n",
        "\n",
        "metrics = model.train(train_data, valid_data, n_epochs=n_epochs,update_fn=update_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Training CoxMLP\n",
            "Finished Training with 2000 iterations in 372.24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cuIXsD64-mDj",
        "colab_type": "code",
        "outputId": "145573bf-10b4-4e80-b351-b99fb8ce52fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model.get_concordance_index(**train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7325826882477129"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "JGUNO3iV4KLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa25d4a6-211a-4723-f22c-b49990cc8922"
      },
      "cell_type": "code",
      "source": [
        "model.get_concordance_index(**valid_data)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7134670487106017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "metadata": {
        "id": "bgn7No-v4KTV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "fabe84fd-5f5e-4008-ae58-521fdc39d4b9"
      },
      "cell_type": "code",
      "source": [
        "deepsurv.plot_log(metrics)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDF+/T9OWmsoCsz/EwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRMQSiTHGmIj8PxkdMwLcSacEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "w0Rl8HFXzflR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rec_train = model.recommend_treatment(train_data['x'], train_data['t'], 2, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5omjGgz2s_cG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rec_valid = model.recommend_treatment(valid_data['x'], valid_data['t'], 2, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KWjCcEFn0dlB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(rec_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "65qUXcnK2kCR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_valid = pd.DataFrame(rec_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "woV6ZRge2sEI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_valid.columns = ['recij_valid']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_l1aoGn0-FH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.columns = ['recij']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hFp_mo7I383B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1906
        },
        "outputId": "fdac84de-e1e6-437d-cb86-54ea71f85272"
      },
      "cell_type": "code",
      "source": [
        "data"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recij</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.869285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.892474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.344195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.420172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.501167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.189242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.870105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.119706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.271060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.150997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-1.075380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-0.124715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-0.149674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-0.618455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.254745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.778465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5.891125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.052148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.942016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.307247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>-0.360303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.444912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.040563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-1.306986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.265359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>-0.316839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.514997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.558630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>-0.847336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>-0.416454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>5.491556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>8.072311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>2.514321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.026399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.744600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-1.006731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>3.941792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>-0.352096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>-0.534943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.181447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>-0.453517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.252488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>-0.630448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>-0.408002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>-0.111158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>-0.616074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-0.195671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.160292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.020958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-0.940466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>-0.614025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>-0.284474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-0.883019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-0.865785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>-0.662806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>-0.528310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>-0.379224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>-0.258191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>-0.807803</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>111 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        recij\n",
              "0   -0.869285\n",
              "1    6.892474\n",
              "2    2.344195\n",
              "3    0.420172\n",
              "4    0.501167\n",
              "5   -0.189242\n",
              "6   -0.870105\n",
              "7    0.119706\n",
              "8    4.271060\n",
              "9   -0.150997\n",
              "10  -1.075380\n",
              "11  -0.124715\n",
              "12  -0.149674\n",
              "13  -0.618455\n",
              "14  -0.254745\n",
              "15  -0.778465\n",
              "16   5.891125\n",
              "17  -0.052148\n",
              "18  -0.942016\n",
              "19  -0.307247\n",
              "20  -0.360303\n",
              "21  -0.444912\n",
              "22   1.040563\n",
              "23  -1.306986\n",
              "24   0.265359\n",
              "25  -0.316839\n",
              "26   0.514997\n",
              "27  -0.558630\n",
              "28  -0.847336\n",
              "29  -0.416454\n",
              "..        ...\n",
              "81   5.491556\n",
              "82   8.072311\n",
              "83   2.514321\n",
              "84   0.026399\n",
              "85  -0.744600\n",
              "86  -1.006731\n",
              "87   3.941792\n",
              "88  -0.352096\n",
              "89  -0.534943\n",
              "90   0.181447\n",
              "91  -0.453517\n",
              "92  -0.252488\n",
              "93  -0.630448\n",
              "94   0.000000\n",
              "95  -0.408002\n",
              "96  -0.111158\n",
              "97  -0.616074\n",
              "98  -0.195671\n",
              "99  -0.160292\n",
              "100  0.020958\n",
              "101 -0.940466\n",
              "102 -0.614025\n",
              "103 -0.284474\n",
              "104 -0.883019\n",
              "105 -0.865785\n",
              "106 -0.662806\n",
              "107 -0.528310\n",
              "108 -0.379224\n",
              "109 -0.258191\n",
              "110 -0.807803\n",
              "\n",
              "[111 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "metadata": {
        "id": "Kl839yHq20RA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "4c04d06a-c1df-4276-b091-c9c6cf78fb87"
      },
      "cell_type": "code",
      "source": [
        "data_valid"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recij_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.807803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.162650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.266880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.144302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.911277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.158394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.936148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1.079177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.136808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.518102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.844846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-0.241372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-0.451834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-1.046089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-1.172029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-1.167958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.489567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.391761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-1.168270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.294483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>-1.033567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5.492441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.455648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.246084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2.993172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5.908227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>-1.106580</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    recij_valid\n",
              "0     -0.807803\n",
              "1     -1.162650\n",
              "2     -0.266880\n",
              "3     -1.144302\n",
              "4     -0.911277\n",
              "5     -0.158394\n",
              "6     -0.936148\n",
              "7     -1.079177\n",
              "8     -0.136808\n",
              "9      0.518102\n",
              "10     1.844846\n",
              "11    -0.241372\n",
              "12    -0.451834\n",
              "13    -1.046089\n",
              "14    -1.172029\n",
              "15    -1.167958\n",
              "16     1.489567\n",
              "17    -0.391761\n",
              "18    -1.168270\n",
              "19    -0.294483\n",
              "20    -1.033567\n",
              "21     5.492441\n",
              "22     0.455648\n",
              "23    -0.246084\n",
              "24     2.993172\n",
              "25     5.908227\n",
              "26    -1.106580"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "metadata": {
        "id": "HCpzvVO51_fs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.loc[data['recij'] >= 0,'recij'] = 1\n",
        "data.loc[data['recij'] < 0,'recij'] = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G7LCm4M-26CZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_valid.loc[data_valid['recij_valid'] >= 0,'recij_valid'] = 1\n",
        "data_valid.loc[data_valid['recij_valid'] < 0,'recij_valid'] = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zziiHEoN2VT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1906
        },
        "outputId": "06ff69d7-2249-444d-fa92-bb0504f6a77b"
      },
      "cell_type": "code",
      "source": [
        "data"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recij</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>111 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     recij\n",
              "0      2.0\n",
              "1      1.0\n",
              "2      1.0\n",
              "3      1.0\n",
              "4      1.0\n",
              "5      2.0\n",
              "6      2.0\n",
              "7      1.0\n",
              "8      1.0\n",
              "9      2.0\n",
              "10     2.0\n",
              "11     2.0\n",
              "12     2.0\n",
              "13     2.0\n",
              "14     2.0\n",
              "15     2.0\n",
              "16     1.0\n",
              "17     2.0\n",
              "18     2.0\n",
              "19     2.0\n",
              "20     2.0\n",
              "21     2.0\n",
              "22     1.0\n",
              "23     2.0\n",
              "24     1.0\n",
              "25     2.0\n",
              "26     1.0\n",
              "27     2.0\n",
              "28     2.0\n",
              "29     2.0\n",
              "..     ...\n",
              "81     1.0\n",
              "82     1.0\n",
              "83     1.0\n",
              "84     1.0\n",
              "85     2.0\n",
              "86     2.0\n",
              "87     1.0\n",
              "88     2.0\n",
              "89     2.0\n",
              "90     1.0\n",
              "91     2.0\n",
              "92     2.0\n",
              "93     2.0\n",
              "94     1.0\n",
              "95     2.0\n",
              "96     2.0\n",
              "97     2.0\n",
              "98     2.0\n",
              "99     2.0\n",
              "100    1.0\n",
              "101    2.0\n",
              "102    2.0\n",
              "103    2.0\n",
              "104    2.0\n",
              "105    2.0\n",
              "106    2.0\n",
              "107    2.0\n",
              "108    2.0\n",
              "109    2.0\n",
              "110    2.0\n",
              "\n",
              "[111 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "metadata": {
        "id": "8fMnkd-I3F9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "13a9b98d-0ea6-41ed-c784-47c655b3bb9f"
      },
      "cell_type": "code",
      "source": [
        "data_valid"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recij_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    recij_valid\n",
              "0           2.0\n",
              "1           2.0\n",
              "2           2.0\n",
              "3           2.0\n",
              "4           2.0\n",
              "5           2.0\n",
              "6           2.0\n",
              "7           2.0\n",
              "8           2.0\n",
              "9           1.0\n",
              "10          1.0\n",
              "11          2.0\n",
              "12          2.0\n",
              "13          2.0\n",
              "14          2.0\n",
              "15          2.0\n",
              "16          1.0\n",
              "17          2.0\n",
              "18          2.0\n",
              "19          2.0\n",
              "20          2.0\n",
              "21          1.0\n",
              "22          1.0\n",
              "23          2.0\n",
              "24          1.0\n",
              "25          1.0\n",
              "26          2.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    }
  ]
}